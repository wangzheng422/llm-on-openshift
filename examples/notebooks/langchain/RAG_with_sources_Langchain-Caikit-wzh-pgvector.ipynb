{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5efb44b5-ffd8-4846-bf43-0baba85c4738",
   "metadata": {},
   "source": [
    "## Example of querying a set of documents with sources using Caikit-TGIS with Llama2, Langchain and a custom prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a196e07-7980-43f3-906e-7373367fe896",
   "metadata": {},
   "source": [
    "### Set the Inference server url (replace with your own address) and the model-id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9aaedb0b-4a37-418e-9608-8abcc819f87a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inference_server_url = \"your_server_address:port\"\n",
    "model_id = \"Llama-2-7b-chat-hf\"\n",
    "\n",
    "inference_server_url = \"caikit-tgis-example-isvc-predictor.kserve-demo.svc.cluster.local:80\"\n",
    "model_id = 'flan-t5-small-caikit'\n",
    "\n",
    "CONNECTION_STRING = \"postgresql+psycopg://vectordb:vectordb@postgresql.pgvector-demo.svc.cluster.local:5432/vectordb\"\n",
    "\n",
    "# If your endpoint is using a self-signed certificate, export the certificate chain as a .pem file and provide its path\n",
    "# Example: certificate_chain = \"certificate.pem\"\n",
    "# Adjust llm instantiation to use this parameter or not\n",
    "certificate_chain_file = \"certificate.pem\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a83a615-62f4-4fcf-924c-a9096a3d511c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Optional, requirements if they are not already present\n",
    "# !pip -q install grpcio grpcio-reflection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fbc3e3-6612-49eb-b07e-710e27f4d2df",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load some data from the folder where we have stored the PDF documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d31f9b89-3613-42fc-aa6f-683ac78accff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "\n",
    "# pdf_folder_path = 'rhods-doc'\n",
    "\n",
    "# loader = PyPDFDirectoryLoader(pdf_folder_path)\n",
    "# docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4547b1-4dea-4c08-ad79-c70186f25611",
   "metadata": {},
   "source": [
    "### Split the data in chunks large enough to have meaningful answers, and some overlap not to miss anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a7a68c1-2308-410e-b50f-9270c00b1d68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1024, chunk_overlap = 40)\n",
    "# all_splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e32e34-aa32-4a57-b4ac-3e2e490b1be6",
   "metadata": {},
   "source": [
    "### Store the data as embeddings in a vector database (Chroma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af325249-120a-49e4-be21-5b902e054819",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unable to connect optimized C data functions [No module named '_testbuffer'], falling back to pure Python\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "# from langchain.vectorstores import Chroma\n",
    "\n",
    "from langchain.vectorstores.pgvector import PGVector\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "vectorstore = PGVector(\n",
    "    connection_string=CONNECTION_STRING,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    embedding_function=embeddings)\n",
    "\n",
    "# vectorstore = Chroma.from_documents(documents=all_splits, embedding=HuggingFaceEmbeddings())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34903f71-99cc-4d4d-a97a-dc530e9c5a47",
   "metadata": {},
   "source": [
    "### Test data retrieving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb47f639-98bc-4f59-b6b6-56e01affb708",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='-\\n, and must start and end with an alphanumeric\\ncharacter.\\n5\\n. \\nEnter a \\ndescription\\n for your data science project.\\n6\\n. \\nClick \\nCreate\\n.\\nA project details page opens. From here, you can create workbenches, add cluster storage, and\\nadd data connections to your project.\\nVerification\\nThe data science project that you created is displayed on the \\nData science projects\\n page.\\nCHAPTER 4. CREATING A DATA SCIENCE PROJECT\\n9', metadata={'source': 'rhods-doc/red_hat_openshift_data_science_self-managed-1.32-getting_started_with_red_hat_openshift_data_science_self-managed-en-us.pdf', 'page': 12}),\n",
       " Document(page_content='-\\n, and must start and end with an alphanumeric\\ncharacter.\\n5\\n. \\nEnter a \\ndescription\\n for your data science project.\\n6\\n. \\nClick \\nCreate\\n.\\nA project details page opens. From here, you can create workbenches, add cluster storage, and\\nCHAPTER 3. WORKING ON DATA SCIENCE PROJECTS\\n11', metadata={'source': 'rhods-doc/red_hat_openshift_data_science_self-managed-1.32-working_on_data_science_projects-en-us.pdf', 'page': 14}),\n",
       " Document(page_content='CHAPTER 4. CREATING A DATA SCIENCE PROJECT\\nTo start your data science work, create a data science project. Creating a project helps you organize your\\nwork in one place. You can also enhance the capabilities of your data science project by adding\\nworkbenches, adding storage to your project’s cluster, adding data connections, and adding model\\nservers.\\nPrerequisites\\nYou have logged in to Red Hat OpenShift Data Science.\\nIf you are using specialized OpenShift Data Science groups, you are part of the user group or\\nadmin group (for example, \\nrhods-users\\n or \\nrhods-admin\\n ) in OpenShift.\\nProcedure\\n1\\n. \\nFrom the OpenShift Data Science dashboard, click \\nData Science Projects\\n.\\nThe \\nData science projects\\n page opens.\\n2\\n. \\nClick \\nCreate data science project\\n.\\nThe \\nCreate a data science project\\n dialog opens.\\n3\\n. \\nEnter a \\nname\\n for your data science project.\\n4\\n. \\nOptional: Edit the \\nresource name\\n for your data science project. The resource name must\\nconsist of lowercase alphanumeric characters, \\n-', metadata={'source': 'rhods-doc/red_hat_openshift_data_science_self-managed-1.32-getting_started_with_red_hat_openshift_data_science_self-managed-en-us.pdf', 'page': 12}),\n",
       " Document(page_content='3.1.1. Creating a data science project\\nTo start your data science work, create a data science project. Creating a project helps you organize your\\nwork in one place. You can also enhance the capabilities of your data science project by adding\\nworkbenches, adding storage to your project’s cluster, adding data connections, and adding model\\nservers.\\nPrerequisites\\nYou have logged in to Red Hat OpenShift Data Science.\\nIf you are using specialized OpenShift Data Science groups, you are part of the user group or\\nadmin group (for example, \\nrhods-users\\n or \\nrhods-admin\\n ) in OpenShift.\\nProcedure\\n1\\n. \\nFrom the OpenShift Data Science dashboard, click \\nData Science Projects\\n.\\nThe \\nData science projects\\n page opens.\\n2\\n. \\nClick \\nCreate data science project\\n.\\nThe \\nCreate a data science project\\n dialog opens.\\n3\\n. \\nEnter a \\nname\\n for your data science project.\\n4\\n. \\nOptional: Edit the \\nresource name\\n for your data science project. The resource name must\\nconsist of lowercase alphanumeric characters, \\n-', metadata={'source': 'rhods-doc/red_hat_openshift_data_science_self-managed-1.32-working_on_data_science_projects-en-us.pdf', 'page': 14})]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# question = \"How do I create a Data Science Project\"\n",
    "# docs = vectorstore.similarity_search(question)\n",
    "# docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f4dd9a-9ac0-407a-9455-0a5906aa9ed3",
   "metadata": {},
   "source": [
    "### Create the chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eda3cceb-2c39-4a74-bf72-a733b22dfd22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "import caikit_tgis_langchain\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "\n",
    "# NOTE: This template syntax is specific to Llama2\n",
    "template=\"\"\"<s>[INST] <<SYS>>\n",
    "You are a helpful, respectful and honest assistant.\n",
    "You will be given a question you need to answer, and a context to provide you with information. You must answer the question based as much as possible on this context.\n",
    "Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
    "\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
    "<</SYS>>\n",
    "\n",
    "Question: {question}\n",
    "Context: {context} [/INST]\n",
    "\"\"\"\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n",
    "\n",
    "llm = caikit_tgis_langchain.CaikitLLM(\n",
    "    inference_server_url=inference_server_url,\n",
    "    model_id=model_id,\n",
    "    certificate_chain=certificate_chain_file,\n",
    "    streaming=True\n",
    ")\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(llm,\n",
    "                                       retriever=vectorstore.as_retriever(),\n",
    "                                       chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT},\n",
    "                                       return_source_documents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03510738-5704-4b56-9ce8-db99d0259757",
   "metadata": {},
   "source": [
    "### Launch the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb26a45d-7953-42cb-aab5-cf2e8c63213d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To create a data science project in Red Hat OpenShift Data Science, follow these steps:\n",
      "1. Log in to your OpenShift Data Science account.\n",
      "2. From the OpenShift Data Science dashboard, click on \"Data Science Projects.\"\n",
      "3. Click on \"Create Data Science Project.\"\n",
      "4. Enter a name for your data science project. The name must consist of lowercase alphanumeric characters, and it must start and end with an alphanumeric character.\n",
      "5. Optional: Edit the resource name for your data science project.\n",
      "6. Click \"Create Data Science Project\" to create your project.\n",
      "Once your project is created, you can add workbenches, add cluster storage, add data connections, and add model servers to enhance the capabilities of your project.\n",
      "Remember to always follow the prerequisites and procedure steps provided in the context to ensure that your project is created successfully."
     ]
    }
   ],
   "source": [
    "question = \"How do I create a Data Science Project?\"\n",
    "result = qa_chain({\"query\": question, \"min_new_tokens\": 100, \"max_new_tokens\": 1024}, callbacks=[StreamingStdOutCallbackHandler()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406b28ac-a3ab-462c-add7-e5473eaea92f",
   "metadata": {},
   "source": [
    "### Print the sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07166717-6b89-47de-baab-f91695602e21",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rhods-doc/red_hat_openshift_data_science_self-managed-1.32-getting_started_with_red_hat_openshift_data_science_self-managed-en-us.pdf\n",
      "rhods-doc/red_hat_openshift_data_science_self-managed-1.32-working_on_data_science_projects-en-us.pdf\n"
     ]
    }
   ],
   "source": [
    "def remove_duplicates(input_list):\n",
    "    unique_list = []\n",
    "    for item in input_list:\n",
    "        if item.metadata['source'] not in unique_list:\n",
    "            unique_list.append(item.metadata['source'])\n",
    "    return unique_list\n",
    "\n",
    "results = remove_duplicates(result['source_documents'])\n",
    "\n",
    "for s in results:\n",
    "    print(s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
